<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>IGTechTeam</title>

    <!-- Favicon -->
    <link
      rel="icon"
      type="image/png"
      href="/static/images/favicon.png"
    />

    <!-- Bootstrap CSS -->
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    />

    <!-- Summernote CSS -->
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/summernote/0.8.18/summernote-lite.min.css"
      rel="stylesheet"
    />

    <!-- Highlight.js -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>

    <!-- Font Awesome CDN -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
    />

    <!-- jQuery & Summernote JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/summernote/0.8.18/summernote-lite.min.js"></script>

    <!-- Bootstrap JS (for navbar toggle) -->
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <style>
      body {
        background-color: #e6f0f7;
        font-family: "Segoe UI", sans-serif;
        display: flex;
        flex-direction: column;
        min-height: 100vh;
      }

      .navbar {
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        position: sticky;
        top: 0;
        z-index: 1000;
        background-color: #1f2c39;
      }

      .navbar-brand {
        font-weight: bold;
        font-size: 1.4rem;
        color: #fff !important;
      }

      .navbar-nav .nav-link {
        color: #f8f9fa !important;
        font-weight: 500;
        padding: 8px 16px;
      }

      .navbar-nav .nav-link:hover {
        background-color: #485768;
        border-radius: 5px;
      }

      .navbar-nav .btn {
        border-radius: 8px;
        transition: all 0.2s ease;
      }

      .navbar-nav .btn:hover {
        transform: scale(1.05);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      }

      .alert-warning {
        margin-top: 1rem;
        font-weight: 500;
        border-left: 6px solid #ffc107;
        background-color: #fff8e1;
      }

      .alert-danger {
        margin-top: 1rem;
      }

      .post-card {
        background-color: #ffffff;
        border-radius: 8px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        transition: all 0.3s ease;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        height: 100%;
        padding: 1rem;
      }

      .post-title {
        font-size: 1.75rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: #343a40;
      }

      /* Images in post content - full width only on mobile */
      @media (max-width: 1024px) {
        .post-content img[style] {
          width: 100% !important;
        }
      }

      .featured-image-container {
        display: flex;
        justify-content: center;
        align-items: center;
        max-width: 100%;
        aspect-ratio: 3 / 2;
        border-radius: 8px;
        overflow: hidden;
      }

      .featured-img {
        width: 100%;
        height: 100%;
        object-fit: contain;
      }

      .badge {
        font-size: 0.875rem;
        font-weight: 600;
        padding: 5px 10px;
      }

      .text-dark {
        text-decoration: none;
      }

      /* Fix maximize fullscreen issue in Summernote */
      .note-editor.fullscreen {
        z-index: 9999 !important;
        position: fixed !important;
        top: 0;
        left: 0;
        width: 100% !important;
        height: 100% !important;
        background: white !important;
        overflow: auto;
      }

      body.note-editor-fullscreen {
        overflow: hidden;
      }

      .social-icons {
        padding-bottom: 10px;
      }

      .social-icons a {
        color: #f8f9fa;
        font-size: 1.5rem;
        margin: 0 10px;
        transition: all 0.3s ease;
      }

      .social-icons a:hover {
        color: #007bff;
        transform: scale(1.2);
      }

      .page-content{
        flex: 1;
      }
    </style>
  </head>
  <body>
    <!-- Navbar -->
    <nav class="navbar navbar-expand-md navbar-light sticky-top">
      <div class="container">
        <a class="navbar-brand" href="/">
          IGTechTeam
        </a>

        <!-- Mobile toggle button -->
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarContent"
          aria-controls="navbarContent"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarContent">
          <div class="navbar-nav mr-auto">
            
            <a class="nav-link" href="/contact.html"
              >Contact</a
            >
            <a class="nav-link" href="/about.html">About me</a>
          </div>

          <div class="navbar-nav ml-auto d-flex align-items-center">
            <a
              class="btn btn-light px-4 py-2 fw-semibold shadow-sm mr-2"
              href="/login.html"
            >
              Login
            </a>

            
          </div>
        </div>
      </div>
    </nav>

    <!-- Site Notice -->
    <div class="container text-center">
      <div class="alert alert-warning" role="alert">
        🚧 Website construction is going on! Please check back later. 🚧
      </div>
    </div>

    <!-- Flash Messages -->
    <div class="container">
      
    </div>

    <!-- Page Content -->
    <div class="container page-content">

<!-- Post Header -->
<header
  class="post-header"
  style="
    background: linear-gradient(135deg, #1d2b64, #f8cdda);
    color: white;
    padding: 2.5rem;
    border-radius: 16px 16px 0 0;
    box-shadow: 0 6px 15px rgba(0, 0, 0, 0.1);
    text-align: center;
  "
>
  <h1
    style="
      font-size: 2.8rem;
      font-weight: bold;
      color: white;
      -webkit-text-stroke: 1px black;
      text-shadow: 2px 2px 6px rgba(0, 0, 0, 0.5);
      letter-spacing: 1px;
      text-align: center;
      margin-bottom: 1rem;
    "
  >
    Face Recognition System Complete Documentation
  </h1>

  <span
    style="
      background: rgba(255, 255, 255, 0.2);
      padding: 0.4rem 1rem;
      border-radius: 20px;
      font-size: 0.95rem;
    "
  >
    🕒 2025-04-27 10:13:59.206333
  </span>
</header>

<!-- Post Body -->
<div
  class="post-content"
  style="
    padding: 2rem;
    background: #ffffff;
    border-radius: 0 0 16px 16px;
    box-shadow: 0 6px 20px rgba(0, 0, 0, 0.05);
    margin-top: -1px;
  "
>
  <p><p style="background-color: rgb(255, 255, 255);"><mark class="has-inline-color has-vivid-red-color" style="background-color: rgba(0, 0, 0, 0);"><font color="#e76363">This is the finalized report on the Face Recognition System project. I have started from the Abstract to the final chapter. I have written this post so that it will be easy for you to understand how to write documentation on any project.&nbsp;</font></mark></p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);">Table of Content</h2><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#abstract">Abstract</a></span></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#figures">List of Figures</a></span></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#abbreviations">List of Abbreviations</a></span></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#chapter-1">Chapter 1: Introduction</a></span></p><p style="background-color: rgb(255, 255, 255);"><a href="#background">1.1 Background</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#problem-statement">1.2 Problem Statement</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#objectives">1.3 Objectives</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#future-scope">1.4 Future scope</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#limitation">1.5 Limitation</a></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#chapter-2">Chapter 2: Requirement Analysis and Feasibility Analysis</a></span></p><p style="background-color: rgb(255, 255, 255);"><a href="#literature-review">2.1 Literature review</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#requirement-analysis">2.2 Requirement analysis</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#feasibility-study">2.3 Feasibility study</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#system-requirements">2.4 Structuring System Requirements</a></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#chapter-3">Chapter 3: Design of Face Recognition System</a></span></p><p style="background-color: rgb(255, 255, 255);"><a href="#approach">3.1 Face Recognition Approach</a></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#chapter-4">Chapter 4: Implementation and Testing</a></span></p><p style="background-color: rgb(255, 255, 255);"><a href="#implementation">4.1 Implementation</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#testing">4.2 Testing</a></p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><a href="#chapter-5">Chapter 5: Conclusion and Future Work</a></span></p><p style="background-color: rgb(255, 255, 255);"><a href="#conclusion">5.1 Conclusion</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#future-work">5.2 Future work</a></p><h2 class="wp-block-heading has-text-align-left" id="abstract" style="background-color: rgb(255, 255, 255);">Abstract</h2><p style="background-color: rgb(255, 255, 255);">Recognizing faces is the easiest way to distinguish the individual identity of each person. Mainly, the human face recognition system has two phases- one is to detect faces and another is to recognize faces. Detection of face means to identify if the images contain any face or not and recognition of face means to remember the name of the detecting face. For this, CNN (Convolutional Neural Network) plays a significant role. There are a lot of neural network libraries like TFlearn and Keras. This project uses TensorFlow and TFlearn. Also, this report is about how to extract audio using&nbsp;<code style="font-size: 14px;">gtts</code>&nbsp;(Google Text to Speech). This library saves what we want to record (e.g. authorized person names). The area of this project's face recognition system is Image processing.</p><p style="background-color: rgb(255, 255, 255);">Keywords: Face Recognition System, OpenCV, CNN, TensorFlow, TFlearn, Tkinter,&nbsp;<code style="font-size: 14px;">gtts</code></p><h2 class="wp-block-heading" id="figures" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">List of Figures:</span></h2><p style="background-color: rgb(255, 255, 255);"><a href="#fig-1.1">Figure 1.1: Steps of face recognition system applications</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-2.4.1">Figure 2.4.1: ER Diagram of Face Recognition System</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-2.4.2(a)">Figure 2.4.2(a): Data flow diagram (level 0)</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-2.4.2(b)">Figure 2.4.2(b): Data Flow Diagram (level 1)</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3">Figure 3: System architecture for Face Recognition</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3.1">Figure 3.1: Face Recognition Approach</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3.1.2">Figure 3.1.2: The framework of face detection process</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3.1.3(a)">Figure 3.1.3(a): The framework of face recognition system</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3.1.3(b)">Figure 3.1.3(b): Multi-layer network structure</a></p><p style="background-color: rgb(255, 255, 255);"><a href="#fig-3.1.3(c)">Figure 3.1.3(c): Activation functions used in neural network</a></p><h2 class="wp-block-heading" id="abbreviations" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">List of ABBREVIATIONS:</span></h2><p style="background-color: rgb(255, 255, 255);">CNN&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Convolutional Neural Network</p><p style="background-color: rgb(255, 255, 255);">CV&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Computer Vision</p><p style="background-color: rgb(255, 255, 255);">GUI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Graphical User Interface</p><p style="background-color: rgb(255, 255, 255);">IDE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Integrated Development Environment</p><p style="background-color: rgb(255, 255, 255);">ReLU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Rectified Linear Unit</p><p style="background-color: rgb(255, 255, 255);">RGB&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Red, Green, Blue</p><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" id="chapter-1" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">Chapter 1: Introduction</span></h2><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span id="background" style="font-weight: bolder;">1.1. Background</span></h2><p style="background-color: rgb(255, 255, 255);">Object detection is the technique of detecting real-world object instances in photos or videos, such as vehicles, televisions, and persons. It allows for the detection, recognition, and localization of objects in an image.</p><p style="background-color: rgb(255, 255, 255);">Face Recognition is the process of determining whether a previously detected object is a known or unknown face. Two major libraries used for this project are TensorFlow and TFlearn. The main aim of this project is to predict the images with their name.</p><p style="background-color: rgb(255, 255, 255);">For any face recognition project, generating a dataset is very important. So, for this, the OpenCV library comes into existence. OpenCV is a free and open-source software library for computer vision and machine learning. To detect and recognize faces may not be as accurate as using deep learning.</p><p style="background-color: rgb(255, 255, 255);">Facial recognition systems are based on computer programs that examine pictures of human faces in order to identify them. The applications take a picture of the face and measure things like the distance between our&nbsp;eyes, the length of the nose, and the angle of our&nbsp;jaw, and create a unique file using that file, it then compares the image with another image and produces a score that measures how similar the images are to each other.</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1bU5HTT_8v_e5SjJfN1keQSQ2588LyjE7&amp;sz=w1000" data-filename="frs-steps.png" style="width: 383px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-1.1" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 1.1: Face recognition system application steps<br></figcaption></figure><h2 class="wp-block-heading" id="problem-statement" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">1.2. Problem Statement</span></h2><p style="background-color: rgb(255, 255, 255);">There might have been a number of situations where it is necessary to recognize faces or simply detect faces. The traditional lock/unlock methods are quite inefficient. There may be a possibility of losing keys or breaching codes/passwords. So, we propose a face recognition system that can be able to recognize faces with the maximum accuracy possible.</p><h2 class="wp-block-heading" id="objectives" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">1.3. Objectives</span></h2><p style="background-color: rgb(255, 255, 255);">The following are the system's objectives:</p><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>Detect faces.</li><li>Match detected faces to the images previously captured and recognize them.</li><li>Provides accurate information about them (e.g. their names).</li></ul><h2 class="wp-block-heading" id="future-scope" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">1.4. Future scope</span></h2><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>Our current recognition system acquires images from files located in a folder and from the webcam. It is possible to add scanner support for more flexibility.</li><li>Currently, our system fails under vastly varying conditions which we can solve in the future.</li></ul><h2 class="wp-block-heading" id="limitation" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">1.5. Limitation</span></h2><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>Insufficient lighting, sunglasses, long hair, low-resolution pictures, or any other object that partially blocks the subject's face will cause the face recognition system to malfunction.</li><li>This system can’t tell the difference between identical twins.</li></ul><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" id="chapter-2" style="background-color: rgb(255, 255, 255);">Chapter 2: Requirement Analysis and Feasibility Analysis</h2><h2 class="wp-block-heading" id="literature-review" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.1. Literature review</span></h2><p style="background-color: rgb(255, 255, 255);">The history of face recognition technology unfolds chronologically, beginning with semi-automated systems in the 1960s. In 1973, Kenade developed fully automated face recognition, extracting 16 facial features for identification. Eigenfaces, introduced by Sirovich and Kirby in 1986, utilized Principal Component Analysis to reconstruct images from lower dimensions.</p><p style="background-color: rgb(255, 255, 255);">Turk and Pentland expanded on Eigenfaces in 1991, enabling face detection within images. DARPA and NIST launched the FERET program in the 1990s, aiming to stimulate commercial face recognition. Facebook introduced face recognition in 2010, despite initial privacy concerns. Techniques continued to evolve, with advancements in criminal identification and Viola-Jones face detection.</p><p style="background-color: rgb(255, 255, 255);">In 2012, Uttam Mande explored criminal identification through facial evidence. A paper in 2014 analyzed the Viola-Jones technique, proposing post-processing for improved efficiency. Indian students introduced a speed-enhancing technique in 2018.</p><p style="background-color: rgb(255, 255, 255);">Overall, face recognition technology has garnered increasing interest for its natural, nonintrusive, and versatile applications.</p><h2 class="wp-block-heading" id="requirement-analysis" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.2. Requirement analysis</span></h2><p style="background-color: rgb(255, 255, 255);">Requirement analysis is a process of carefully identifying, specifying, and documenting the numerous requirements that are relevant to a certain business purpose. Requirements gathering helps in clearly understanding the needs of the customer, defining the scope of the project, and the resources required to complete it. This project's functional, non-functional, and technological needs are as follows:</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.2.1. Functional requirements:</span></p><p style="background-color: rgb(255, 255, 255);">The functional requirement refers to "any requirement which specifies what the system should do". The following is a list of the project's functional requirements:</p><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>It should support images in the 'PNG' and 'JPEG' formats.</li><li>It should generate the dataset properly.</li><li>It should be able to anticipate authorized users with high accuracy.</li></ul><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.2.2. Non-functional requirements:</span></p><p style="background-color: rgb(255, 255, 255);">The non-functional requirement refers to “any requirement that specifies how the system performs a certain function”. They are the characteristics or attributes of the system that can judge its operation. The following is a list of the project's non functional requirements:</p><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>The system's graphical user interface (GUI) will be user-friendly.</li><li>The system will be flexible to changes, e.g. at any time, we should be able to add authorized users.</li><li>The efficiency and effectiveness of the system is important.</li></ul><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.2.3. Technical requirements:</span></p><p style="background-color: rgb(255, 255, 255);">The following are the technical criteria for this project:</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">a.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hardware Requirements</span>:</p><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>camera integrated system</li></ul><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">b.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="font-weight: bolder;">Software Requirements:</span></p><ul class="wp-block-list" style="background-color: rgb(255, 255, 255);"><li>Windows operating system</li><li>TensorFlow version 1.14</li><li>OpenCV</li></ul><h2 class="wp-block-heading" id="feasibility-study" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.3. Feasibility study</span></h2><p style="background-color: rgb(255, 255, 255);">A feasibility study examines the likelihood of a project's success. It also serves as a solid foundation for developing our business plan. This study includes the following three factors:</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.3.1. Technical feasibility</span></p><p style="background-color: rgb(255, 255, 255);">Technical feasibility defines the feasibility that is concerned with specifying equipment and software that will successfully satisfy the user requirement. For this project, the software and hardware required for the development of this system are already available as free as open source, so this project is technically feasible.</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.3.2. Economic feasibility</span></p><p style="background-color: rgb(255, 255, 255);">Economic feasibility means whether a business or a project is feasible cost-wise or logistically. For this project, we download all the requirements from the Internet without any associated charges, making this system economically feasible.</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.3.3. Operational feasibility</span></p><p style="background-color: rgb(255, 255, 255);">Operational feasibility is a measure of how well a new system under consideration will work in solving problems. In this project, the system solves the problem of&nbsp;losing keys or breaching codes/passwords as mentioned in the problem statement, so this project is also operationally feasible.</p><h2 class="wp-block-heading" id="system-requirements" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.4. Structuring System Requirements</span></h2><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.4.1. Data modeling</span></p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1NRI9TmfZvH4P_Tykn5dqAqieRXtvVgIT&amp;sz=w1000" data-filename="er-diagram.png" style="width: 600px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-2.4.1" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 2.4.1: ER Diagram of Face Recognition System</figcaption></figure><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);"></p><figure class="wp-block-image aligncenter size-full" id="fig-2.4.1" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption"><br></figcaption></figure><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">2.4.2. Process modeling</span></p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=18w-JveQkdfrWMmYcGIuhrEqBNCU_5cj7&amp;sz=w1000" data-filename="frs-dfd.png" style="width: 451px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-2.4.2(a)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 2.4.2(a): Data flow diagram (level 0)</figcaption></figure><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);"></p><figure class="wp-block-image aligncenter size-full" id="fig-2.4.2(a)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;"><br></figcaption></figure><div style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1WKHp6TkRg2WMSPZAfyXtqRz_bdJcPAyX&amp;sz=w1000" data-filename="frs-dfd-l1.png" style="font-size: 1rem; width: 562px;"></div><figure class="wp-block-image aligncenter size-full" id="fig-2.4.2(b)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 2.4.2(b): Data Flow Diagram (level 1)<br></figcaption></figure><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" id="chapter-3" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><br></span></h2><h2 class="wp-block-heading" id="chapter-3" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">Chapter 3: Design of Face Recognition System</span></h2><p style="background-color: rgb(255, 255, 255);">This system is mainly based on face detection and face recognition. Among the many possible approaches, we have decided to use the Haar-Like features algorithm for the face detection part and the neural network approach for the face recognition part.</p><p style="background-color: rgb(255, 255, 255);">When the user opens the GUI, there is a button to capture images where the user captures images from the camera. After capturing the image, we preprocess it, detect faces, perform recognition, and then display the result to the user.</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1a9SwS6mUdiGzfmkprTQNZLrVbRJY7oRm&amp;sz=w1000" data-filename="frs-architecture.png" style="width: 441px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-3" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3: System Architecture for Face Recognition<br></figcaption></figure><h2 class="wp-block-heading" id="approach" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;"><br></span></h2><h2 class="wp-block-heading" id="approach" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">3.1. Face Recognition Approach</span></h2><p style="background-color: rgb(255, 255, 255);">The figure below illustrates the approach of the face recognition system:</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1iOTqGWyd7wIjkaEfYzPlqZid6uQC-eAy&amp;sz=w1000" data-filename="frs-approach.png" style="width: 600px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-3.1" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3.1: Face Recognition Approach<br></figcaption></figure><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">3.1.1 Input part</span></h2><p style="background-color: rgb(255, 255, 255);">The input section of the facial recognition system is crucial. Here, the image acquisition procedure takes place, converting real-time images into digital data for processing. The captured images undergo processing through a face detection algorithm.</p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">3.1.2 Face Detection Part</span></h2><p style="background-color: rgb(255, 255, 255);">Face detection is the process of identifying human faces in an image. Below is the process of face detection:</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img selected="" src="https://drive.google.com/thumbnail?id=1hNWLwb0u1TZ5LVeaTLfJVG2WVf0nhSRb&amp;sz=w1000" data-filename="frs-framework.png" style="width: 581px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-3.1.2" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3.1.2: The framework of the face detection process<br></figcaption></figure><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">3.1.3 Face Recognition Part</span></h2><p style="background-color: rgb(255, 255, 255);">Face recognition is a process of recognizing the name of a detected face. Below is the process of face recognition:</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1RBRX7zMqG2wZU_pqqwmhugqXPGuPUpzt&amp;sz=w1000" data-filename="framework-recognition-part.png" style="width: 278px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-3.1.3(a)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3.1.3(a): The framework of the face recognition system</figcaption></figure><p style="background-color: rgb(255, 255, 255);">This report&nbsp;introduces a new face recognition approach in which local features are fed into a neural network.</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1VeAHhvhJdcNZ4P-banUBpov43-qRXj_-&amp;sz=w1000" data-filename="multi-layer-network.png" style="width: 299px;"></p><figure class="wp-block-image aligncenter size-full" id="fig-3.1.3(b)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3.1.3(b): Multi-layer network structure<br></figcaption></figure><p style="background-color: rgb(255, 255, 255);">The activation functions decide which signal to pass to the next neuron. Common activation functions are Linear, Sigmoid, SoftMax, and ReLU functions.</p><p style="background-color: rgb(255, 255, 255); text-align: center;"><img src="https://drive.google.com/thumbnail?id=1QqZwq5N56WIKx1uEQdtNAOBNC4bJQsZI&amp;sz=w1000" data-filename="activation-function.png" style="width: 526px;"><br></p><figure class="wp-block-image aligncenter size-full" id="fig-3.1.3(c)" style="background-color: rgb(255, 255, 255);"><figcaption class="wp-element-caption" style="text-align: center;">Figure 3.1.3(c): Activation functions used in neural network<br></figcaption></figure><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">3.1.4 Output part</span></h2><p style="background-color: rgb(255, 255, 255);">This part is the final step of the face recognition system. The face recognition system determines the person's name by utilizing the output vector of the neural network.</p><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" id="chapter-4" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">Chapter 4: Implementation and Testing</span></h2><h2 class="wp-block-heading" id="implementation" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.1. Implementation</span></h2><p style="background-color: rgb(255, 255, 255);">The implementation phase entails putting the project strategy into practice. This phase begins once the majority of the code for the program is written. It involves translating the requirements specified in the requirement phase into a logical structure that can be implemented in a programming language. We can write code in Jupyter Notebook as an integrated development environment (IDE). This emulator aided in the implementation of the project in an authentic environment.</p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.1.1 Tools used</span></h2><p style="background-color: rgb(255, 255, 255);">1. Camera-integrated system</p><p style="background-color: rgb(255, 255, 255);">We can use either internal or external cameras for this project.</p><p style="background-color: rgb(255, 255, 255);">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jupyter notebook</p><p style="background-color: rgb(255, 255, 255);">We used Jupyter Notebook as an integrated development environment (IDE). It is very easy to use and also flexible because as its name suggests, we can write notes and execute code in the same IDE. We can also use other IDs like sublime text, PyCharm, and so on.</p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.1.2 Libraries used</span></h2><p style="background-color: rgb(255, 255, 255);">1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenCV</p><p style="background-color: rgb(255, 255, 255);">OpenCV is an open-source computer vision and machine learning free software library. This library contains over 2,500 efficient algorithms for detecting and recognizing faces as well as generating datasets.</p><p style="background-color: rgb(255, 255, 255);">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TensorFlow:</p><p style="background-color: rgb(255, 255, 255);">TensorFlow is Google’s Open Source Machine Learning Framework for dataflow programming across a range of tasks. We can use TensorFlow with TFlearn to train the model and to make predictions.</p><p style="background-color: rgb(255, 255, 255);">3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TFlearn</p><p style="background-color: rgb(255, 255, 255);">TFlearn is a Python modular library developed on top of TensorFlow. It is a deep-learning library. We can use either Keras or TFlearn as a TensorFlow framework.</p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.1.3 Data Collection</span></h2><p style="background-color: rgb(255, 255, 255);">Here, we can collect the data in the form of Images. For data collection, we used Python openCV. It combines the best qualities of OpenCV, C++, API, and Python language. OpenCV provides a wide range of computer vision and machine learning methods. We utilize the haarcascade_frontalface_default.xml file to detect frontal faces, crop them to a specific size, and save them in a folder. We can used cropped images for training the model later on.</p><p style="background-color: rgb(255, 255, 255);">Detection = cv2.CascadeClassifier(“haarcascade_frontalface_default.xml”)</p><h2 class="wp-block-heading" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.1.4 CNN Recognition Algorithm</span></h2><p style="background-color: rgb(255, 255, 255);">CNNs are a sort of multi-layer feed-forward neural network. CNNs are made up of filters, kernels, or neurons with learnable weights, parameters, and biases. Each filter takes certain inputs, does convolution, and then optionally adds non-linearity. CNN has Convolutional, Pooling, Rectified Linear Units (ReLU), and Fully Connected Layers.</p><p style="background-color: rgb(255, 255, 255);">a.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We first resized the input image and then passed it to the different conv layers and the pooling layers.</p><p style="background-color: rgb(255, 255, 255);">b.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We have used a 5x5 filter and the number of filters used can differ from layer to layer of convolution.</p><p style="background-color: rgb(255, 255, 255);">c.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally, we get a flattened image which is used later on to recognize the face.</p><h2 class="wp-block-heading" id="testing" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.2. Testing</span></h2><p style="background-color: rgb(255, 255, 255);">Software testing is the practice of evaluating a software product to find inconsistencies between the input that has been provided and the desired result. We generally perform two software tests. They are:</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.2.1 Integration testing</span></p><p style="background-color: rgb(255, 255, 255);">In integration testing, various modules have been combined and tested as a group. &nbsp;The integration testing of the app has been done by dividing the project into two modules. The first module includes a GUI interface working. We check whether the GUI is able to capture the image or not as well as whether it has a proper linkup with the ML model or not. The second module includes whether the ML model is properly trained or not and whether it is able to give output with the best possible accuracy.</p><p style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">4.2.2 System testing</span></p><p style="background-color: rgb(255, 255, 255);">Finally, we conducted testing on a complete integrated system and we verified that our whole system is working correctly and is ready to deploy.</p><p style="background-color: rgb(255, 255, 255);"></p><h2 class="wp-block-heading" id="chapter-5" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">Chapter 5: Conclusion and Future Work</span></h2><p id="conclusion" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">5.1. Conclusion:</span></p><p style="background-color: rgb(255, 255, 255);">Face recognition systems recognize the faces of authorized users very easily. Those persons who want to use the Face recognition system don’t have to know how to make the system but it is sufficient to know how to use it only.</p><p style="background-color: rgb(255, 255, 255);">The main steps of this project are concluded below:</p><p style="background-color: rgb(255, 255, 255);">1: Create a database of authorized users<br>2: Train the model with that dataset<br>3: Calculate the precision (accuracy)<br>4: Predict faces using that trained model<br>5: Represent the project in GUI and also can be able to listen to the names of authorized users.</p><p style="background-color: rgb(255, 255, 255);">All the above-mentioned steps are accomplished successfully. It met our initial aims and objectives and as mentioned in the limitation, we are working to deal with this too.</p><p id="future-work" style="background-color: rgb(255, 255, 255);"><span style="font-weight: bolder;">5.2. Future work:</span></p><p style="background-color: rgb(255, 255, 255);">Still, few faces are there which are really tough to distinguish, for example, twin siblings. Building a system that can distinguish between the two look-alikes can be a challenge. Moreover, we want our system to recognize someone, even though there is only one picture of that person on file, and it was taken at a different angle, in different lighting, or they were wearing sunglasses like a human being. Maybe in the near future, we can imagine our system to identify more challenging images.</p><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);"></p><p style="background-color: rgb(255, 255, 255);">Thus, this face recognition system can be preserved for future use. The dataset can be updated in time to maintain the accuracy. With the help of GUI, it doesn’t take too much time to update the dataset.</p></p>
</div>

<!-- Post Footer -->
<footer
  class="post-footer"
  style="
    margin-top: 2rem;
    background: #f1f3f6;
    padding: 2rem;
    border-radius: 16px;
    text-align: center;
    font-size: 0.95rem;
    color: #444;
    box-shadow: 0 -4px 10px rgba(0, 0, 0, 0.05);
  "
>
  <p><strong>Author:</strong> Ishwar Gautam</p>

  <!-- Buttons -->
  <div style="margin: 1rem 0">
    <a
      href="/"
      class="btn btn-outline-primary"
      style="margin-right: 1rem"
      >← Back to Home</a
    >
    <a
      href="#"
      onclick="window.scrollTo({top: 0, behavior: 'smooth'});"
      class="btn btn-primary"
      >↑ Back to Top</a
    >
  </div>

  <!-- Social Icons -->
  <div class="social-icons" style="margin-top: 1.5rem">
    <a
      href="https://www.facebook.com/igtechteam/"
      target="_blank"
      style="margin: 0 10px; font-size: 1.5rem; color: #3b5998"
      ><i class="fab fa-facebook"></i
    ></a>
    <a
      href="https://www.instagram.com/ishwar_gautam1/"
      target="_blank"
      style="margin: 0 10px; font-size: 1.5rem; color: #e4405f"
      ><i class="fab fa-instagram"></i
    ></a>
    <a
      href="https://github.com/ishwargautam"
      target="_blank"
      style="margin: 0 10px; font-size: 1.5rem; color: #333"
      ><i class="fab fa-github"></i
    ></a>
    <a
      href="https://www.linkedin.com/in/ishwargautam1"
      target="_blank"
      style="margin: 0 10px; font-size: 1.5rem; color: #0077b5"
      ><i class="fab fa-linkedin"></i
    ></a>
    <a
      href="https://www.tiktok.com/@ishwar.gautam1?_t=8pdEalQ0Nn7&_r=1"
      target="_blank"
      style="margin: 0 10px; font-size: 1.5rem; color: #000"
      ><i class="fab fa-tiktok"></i
    ></a>
  </div>

  <!-- YouTube Subscribe -->
  <div style="margin-top: 1.5rem">
    <a
      class="youtube-btn"
      href="https://www.youtube.com/channel/UC4Wf9XNPsbXwQqfKlUZfmtw?sub_confirmation=1"
      target="_blank"
      style="
        background: #ff0000;
        color: white;
        padding: 0.6rem 1.2rem;
        border-radius: 30px;
        text-decoration: none;
        font-weight: bold;
        transition: background 0.3s ease;
      "
    >
      🔔 Subscribe on YouTube
    </a>
  </div>
</footer>

</div>

    <!-- Footer -->
    
<!-- Leaving this empty so to skip base.html footer-->


    <!-- Summernote Initialization -->
    <script>
      $(document).ready(function () {
        $("#summernote").summernote({
          height: 300,
          toolbar: [
            ["style", ["style"]],
            ["font", ["bold", "italic", "underline", "clear"]],
            ["fontname", ["fontname"]],
            ["fontsize", ["fontsize"]],
            ["color", ["color"]],
            ["para", ["ul", "ol", "paragraph"]],
            ["insert", ["link", "picture", "video", "table", "hr"]],
            ["view", ["fullscreen", "codeview", "help"]],
            ["custom", ["codeblock"]],
          ],
          buttons: {
            codeblock: function (context) {
              var ui = $.summernote.ui;
              var button = ui.button({
                contents: '<i class="fa fa-code"></i> Code Block',
                tooltip: "Insert Code Block",
                click: function () {
                  let lang = prompt(
                    "Enter language (e.g. python, javascript, java, html, css):",
                    "python"
                  );
                  if (!lang) lang = "plaintext";

                  var codeBlock = `
                <pre><code class="language-${lang}" contenteditable="true" style="white-space: pre-wrap;"></code></pre>
              `;
                  context.invoke("editor.pasteHTML", codeBlock);
                  hljs.highlightAll();
                  var codeBlockNode =
                    context.invoke("editor.getBody").lastChild;
                  $(codeBlockNode).find("code").focus();
                },
              });
              return button.render();
            },
          },
        });

        // Auto-highlight on Enter inside code blocks
        $(".note-editable").on("keyup", function (e) {
          if (e.key === "Enter") {
            $(this)
              .find("pre code")
              .each(function () {
                hljs.highlightElement(this);
              });
          }
        });

        hljs.highlightAll();
      });
    </script>
  </body>
</html>